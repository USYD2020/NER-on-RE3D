{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "A2_best_model-group-46.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TUXMfG9BVfo_"
      },
      "source": [
        "# COMP5046 Assignment 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mll02uEwWFxf"
      },
      "source": [
        "##Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUKhX08tYfaj"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction import DictVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.linear_model import Perceptron\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.linear_model import PassiveAggressiveClassifier\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGumkx4pYB-d"
      },
      "source": [
        "**Read raw data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrAZxuefWGhj"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        "id = '10rWPMnFgLkVTbmDGzukP7ZLtBEN7AZdh'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('train.csv')  \n",
        "\n",
        "raw_training_df = pd.read_csv('train.csv')\n",
        "\n",
        "id = '1y7EDO2Thqn5mtsbQgx-YjAvWzkx9xqvn'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('test_without_lables.csv')  \n",
        "\n",
        "testing_df = pd.read_csv('test_without_lables.csv')\n",
        "\n",
        "id = '18lUFTMp7PxIBjqU8ImfMrymRJeysljjl'\n",
        "downloaded = drive.CreateFile({'id':id})\n",
        "downloaded.GetContentFile('val.csv')  \n",
        "\n",
        "raw_val_df = pd.read_csv('val.csv')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VlFXvTx29rLp"
      },
      "source": [
        "**Shorten Sentence**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rJZnq24ykMyd",
        "outputId": "7edcf966-1959-4cdf-9401-05e788b36b67"
      },
      "source": [
        "MAX_LENGTH = 64 # max no. words for each sentence.\n",
        "OVERLAP = 20 # if a sentence exceeds MAX_LENGTH, we split it to multiple sentences with overlapping\n",
        "\n",
        "def shorten_sentences(df):\n",
        "    short_sentences = []\n",
        "    short_labels = []\n",
        "    for i, sentence, sentence_label in df[['sents','labels']].itertuples():\n",
        "        words = sentence.split()\n",
        "        labels = sentence_label.split()\n",
        "        if len(words) > MAX_LENGTH:\n",
        "            for p in range(0, len(words), MAX_LENGTH - OVERLAP):\n",
        "                short_sentences.append(' '.join(words[p:p+MAX_LENGTH]))\n",
        "                short_labels.append(' '.join(labels[p:p+MAX_LENGTH]))\n",
        "        else:\n",
        "            short_sentences.append(sentence)\n",
        "            short_labels.append(sentence_label)\n",
        "    return pd.DataFrame({\"sents\":short_sentences, \"labels\":short_labels})\n",
        "\n",
        "# sentence is shorten\n",
        "\n",
        "training_df = shorten_sentences(raw_training_df)\n",
        "print(f'==================== No. training rows: {len(training_df)}=======================')\n",
        "print(\"The max length of trainging sentences is :\", max(map(lambda k : len(k.split()),training_df['sents'])))\n",
        "\n",
        "\n",
        "val_df = shorten_sentences(raw_val_df)\n",
        "print(f'\\n==================== No. validation rows: {len(val_df)}=======================')\n",
        "print(\"The max length of validation sentences is :\", max(map(lambda k : len(k.split()),val_df['sents'])))\n",
        "\n",
        "\n",
        "print(f'\\n==================== No. testing rows: {len(testing_df)}=======================')\n",
        "print(\"The max length of testing sentences is :\", max(map(lambda k : len(k.split()),testing_df['sents'])))\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "==================== No. training rows: 586=======================\n",
            "The max length of trainging sentences is : 64\n",
            "\n",
            "==================== No. validation rows: 196=======================\n",
            "The max length of validation sentences is : 64\n",
            "\n",
            "==================== No. testing rows: 199=======================\n",
            "The max length of testing sentences is : 65\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlhvoZxM95So"
      },
      "source": [
        "**Tokenise sentences into word tokens**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pKLjnheK02Lq"
      },
      "source": [
        "train_data, target_y_train = list(training_df['sents']),list(training_df['labels'])\n",
        "validation_data, target_y_validation = list(val_df['sents']),list(val_df['labels'])\n",
        "test_data = list(testing_df['sents'])\n",
        "\n",
        "uncased_training_data = [text.split() for text in train_data]\n",
        "uncased_val_data = [text.split() for text in validation_data]\n",
        "uncased_test_data = [text.split() for text in test_data]\n",
        "all_uncased_data = uncased_training_data + uncased_val_data + uncased_test_data\n",
        "\n",
        "train_data = [text.split() for text in train_data]\n",
        "target_y_train = [text.split() for text in target_y_train]\n",
        "validation_data = [text.split() for text in validation_data]\n",
        "target_y_validation = [text.split() for text in target_y_validation]\n",
        "test_data = [text.split() for text in test_data]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dDx691GkWNB1"
      },
      "source": [
        "##Input Embedding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKJp8CNpFwhj"
      },
      "source": [
        "**Semantic Textual Feature Embedding**\n",
        "- Word Embedding: WordtoVec"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_AiShsLkGQLy"
      },
      "source": [
        "import gensim.downloader as api\n",
        "word_emb_model4 = api.load(\"glove-wiki-gigaword-50\")\n",
        "\n",
        "# word to vector embedding example\n",
        "# word_emb_model4['iraq']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boduOT8lvtf7"
      },
      "source": [
        "**Syntactic Textual Feature Embedding**\n",
        "- PoS tag\n",
        "- Dependency path\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-ONPXx7jGf1"
      },
      "source": [
        "# Get PoS tagging and dependency information about a word from spacy\n",
        "import spacy\n",
        "from spacy.tokenizer import Tokenizer\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "nlp.tokenizer = Tokenizer(nlp.vocab)"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ALgDZ9ILEn50",
        "outputId": "c22d411c-4c86-48bd-adc1-d106674205a6"
      },
      "source": [
        "!pip install pymagnitude"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pymagnitude in /usr/local/lib/python3.7/dist-packages (0.1.143)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wsi50XpUEqCY"
      },
      "source": [
        "# Get the vector representations of PoS tagging and dependency information from pymagnitude\n",
        "from pymagnitude import *\n",
        "pos_vectors = FeaturizerMagnitude(300, namespace = \"PartsOfSpeech\")\n",
        "dependency_vectors = FeaturizerMagnitude(300, namespace = \"SyntaxDependencies\")\n",
        "vectors = Magnitude(pos_vectors, dependency_vectors) # concatenate word2vec with pos and dependencies"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j8AYJJLE58z"
      },
      "source": [
        "**Domain Feature - Case features represented as a list**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zy2Ah897uc-F"
      },
      "source": [
        "# Customise case feature embedding of a word\n",
        "import re\n",
        "def case_features(text):\n",
        "    # return case feature of a work\n",
        "    return [\n",
        "        int(text[0].isupper()), # Start with capital letter\n",
        "        int(text.upper() == text), # All capital letters\n",
        "        int(text.lower() == text), # All lower letters\n",
        "        int(all([ch.isdigit() for ch in text])), # all digits\n",
        "        int(bool((re.match('^(?=.*[0-9]$)(?=.*[a-zA-Z])',text)))), # 'is_alphanumeric' \n",
        "        1 if '-' in text else 0  #'word_has_hyphen'\n",
        "        ]"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UaL991lIjh7H"
      },
      "source": [
        "# Embedding Choice for Sentence-Level Embedding\n",
        "Embedding can be customised by choosing the desirable arguments.\n",
        "\n",
        "Syntactic feature\n",
        "* w1: word-level PoS embedding from Spacy\n",
        "* w2: word-level dependy embedding from Spacy\n",
        "\n",
        "Domain feature\n",
        "* w3: word-level case features embedding\n",
        "\n",
        "Only w1 and w3 are chosen as elements of sentence level embedding for best model.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TLTEYGDiKgcj"
      },
      "source": [
        "import torch\n",
        "import numpy\n",
        "\n",
        "# Get word2vec embedding from glove,  token PoS tag embedding and dependency embedding from Space\n",
        "def get_sentence_embedding(all_uncased_data, embedding_choice):\n",
        "  dependency_embedding = {}\n",
        "  pos_embedding = {}\n",
        "  case_embedding = {}\n",
        "  sentence_embeds = {}\n",
        "\n",
        "  train_dp = all_uncased_data\n",
        "\n",
        "  for sentence in train_dp:\n",
        "    sentence = ' '.join(sentence)\n",
        "    doc = nlp(sentence)\n",
        "\n",
        "    embeds = []\n",
        "\n",
        "    for token in doc:\n",
        "      # Word-level syntactic/domain features\n",
        "      pos_embedding[token.text] = token.pos_\n",
        "      dependency_embedding[token.text] = token.dep_\n",
        "      case_embedding[token.text] = case_features(token.text)\n",
        "\n",
        "      w1 = torch.tensor([pos_vectors.query(token.pos_)]).float()\n",
        "      w2 = torch.tensor([case_features(token.text)]).float()\n",
        "      w3 = torch.tensor([dependency_vectors.query(token.dep_)]).float()\n",
        "\n",
        "      if embedding_choice == [\"PoS\"]:\n",
        "        embeds.append(numpy.array(w1).tolist())\n",
        "      elif embedding_choice == [\"PoS\",\"case_feature\"]:\n",
        "        embeds.append(numpy.array(torch.cat((w1,w2),1).tolist()[0]))\n",
        "      elif embedding_choice == [\"PoS\",\"dependency\"]:\n",
        "        embeds.append(numpy.array(torch.cat((w1,w3),1).tolist()[0]))\n",
        "      elif embedding_choice == [\"PoS\",\"case_feature\",'dependency']:\n",
        "        embeds.append(numpy.array(torch.cat((w1,w2,w3),1).tolist()[0]))\n",
        "\n",
        "    sentence_embeds[sentence] = np.array(embeds)\n",
        "  \n",
        "  # print(\"DP length:\", len(dependency_embedding))\n",
        "  # print(\"PoS length:\", len(pos_embedding))\n",
        "  # print(\"Case length:\", len(case_embedding))\n",
        "  return sentence_embeds"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LBZXkoylvbs"
      },
      "source": [
        "# Pos + case features + dependency of the word are the possible candidates for the sentence level embedding\n",
        "all_choices = [\"PoS\",\"case_feature\",'dependency']\n",
        "# possible combinations are [0],[0,1],[0,2],[0,1,2] \n",
        "\n",
        "# Change this line to choose your desired word-level embedding choices\n",
        "sentence_embedding_choices = [\"PoS\",\"case_feature\"]"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjIQxAoGMtK7"
      },
      "source": [
        "sentence_embeds = get_sentence_embedding(all_uncased_data, sentence_embedding_choices)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "724nistCu8Sj"
      },
      "source": [
        "**Map word/tag in training/validation/test data to index**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZXgK8fvt6jN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f44834-9c66-4445-b6a3-064cf23fda62"
      },
      "source": [
        "# Map word to index and index to word\n",
        "ix_to_word = {}\n",
        "word_to_ix = {}\n",
        "for sentence in all_uncased_data:\n",
        "    for word in sentence:\n",
        "        # word = word.lower()\n",
        "        if word not in word_to_ix:\n",
        "            word_to_ix[word] = len(word_to_ix)\n",
        "            ix_to_word[word_to_ix[word]] = word\n",
        "\n",
        "# Store all words in word_list\n",
        "word_list = list(word_to_ix.keys())\n",
        "\n",
        "# Map word to index and index to word\n",
        "START_TAG = \"<START>\"\n",
        "STOP_TAG = \"<STOP>\"\n",
        "tag_to_ix = {START_TAG:0, STOP_TAG:1}\n",
        "ix_to_tag = {}\n",
        "for tags in target_y_train+target_y_validation:\n",
        "    for tag in tags:\n",
        "        if tag not in tag_to_ix:\n",
        "            tag_to_ix[tag] = len(tag_to_ix)\n",
        "            ix_to_tag[tag_to_ix[tag]] = tag\n",
        "\n",
        "# Check word_list length\n",
        "len(word_list)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4291"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArZtHUV4Epjt"
      },
      "source": [
        "**Construct embedding matrix for word embedding**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YvfdGFNAt8ek",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "752c35a1-664c-4c7a-f7d1-8324e5106916"
      },
      "source": [
        "import torch\n",
        "\n",
        "word_embedding_matrix = []\n",
        "for i in range(len(word_list)):\n",
        "    word = word_list[i]\n",
        "\n",
        "    if word.lower() in word_emb_model4:\n",
        "      v = word_emb_model4.wv[word.lower()]\n",
        "      w1 = torch.tensor([v.tolist()]).float()\n",
        "    # word embedding for OOV\n",
        "    else:\n",
        "      w1 = torch.tensor([[0]*50]).float()\n",
        "\n",
        "    word_embedding_matrix.append(w1.tolist()[0])\n",
        "\n",
        "\n",
        "word_embedding_matrix = np.array(word_embedding_matrix)\n",
        "word_embedding_matrix.shape"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4291, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epeY0Kt0H34N"
      },
      "source": [
        "**Map all words data to indices data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nv55XoWt9vF"
      },
      "source": [
        "def word_to_index(data, to_ix, tag=False):\n",
        "    input_index_list = []\n",
        "    for sent in data:\n",
        "        if tag:\n",
        "            input_index_list.append([to_ix[w] for w in sent])\n",
        "        else:\n",
        "            input_index_list.append([to_ix[w] for w in sent])\n",
        "    return input_index_list\n",
        "\n",
        "train_input_index =  word_to_index(uncased_training_data,word_to_ix)\n",
        "train_output_index = word_to_index(target_y_train,tag_to_ix,True)\n",
        "val_input_index = word_to_index(uncased_val_data,word_to_ix)\n",
        "val_output_index = word_to_index(target_y_validation,tag_to_ix,True)\n",
        "test_uncased_input_index  = word_to_index(uncased_test_data,word_to_ix)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nC81cKL8XlW4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78112ce2-224a-402a-862d-50d72a5b678f"
      },
      "source": [
        "print(\"word to index example\", word_to_ix['Abdulbaset'])\n",
        "print(\"test input index example\", test_uncased_input_index[:10])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "word to index example 3265\n",
            "test input index example [[1179, 3752, 1906, 49, 138, 184, 140, 35, 20, 193, 372, 2609, 53, 40, 494, 33, 2089, 704, 1063, 33, 431, 435, 40, 33, 213, 90, 340, 25, 97, 1064, 27, 118, 377, 155, 764, 31, 2699, 33, 1169, 84, 53, 2919, 31, 33, 183, 44], [3753, 3754, 24, 33, 46, 3755, 350, 3756, 53, 3757, 53, 3758, 53, 3759, 53, 40, 3760, 3761, 777, 479, 47, 20, 3762, 3763, 3764, 44], [45, 768, 680, 35, 33, 3725, 166, 470, 639, 27, 2294, 53, 3765, 44, 3315, 44, 79, 3404, 3006, 53, 1784, 53, 415, 33, 3766, 1095, 44], [92, 92, 92, 519, 96, 1864, 158, 53, 96, 107, 3767, 31, 3559, 33, 543, 26, 59, 391, 635, 486, 53, 92, 92, 493, 160, 53, 2258, 59, 33, 184, 3768, 1568, 3769, 44, 92], [1516, 3770, 684, 20, 3771, 49, 3772, 27, 20, 2563, 1385, 35, 33, 399, 79, 2266, 53, 3649, 1841, 2980, 3773, 160, 44], [45, 229, 148, 1976, 31, 3729, 3774, 3775, 53, 1745, 352, 3776, 40, 74, 3777, 27, 206, 33, 1800, 34, 35, 3778, 40, 3779, 35, 3780, 44], [2301, 33, 1907, 398, 35, 3781, 40, 603, 3059, 35, 999, 2558, 27, 118, 53, 220, 273, 220, 33, 3782, 927, 3783, 27, 294, 53, 99, 148, 20, 1037, 2958, 31, 192, 31, 1030, 3466, 340, 79, 20, 1752, 53, 40, 3784, 53, 3785, 31, 405, 44], [3003, 5, 2634, 2845, 769, 3005, 2374, 3007, 3008, 910, 2845, 769, 3005, 3786, 3007, 3010, 777, 2634, 44, 3787, 2845, 3008, 910, 44, 3788, 2845, 3010, 777, 2634, 44, 3787, 894, 910, 44, 3788], [45, 583, 35, 340, 341, 27, 294, 40, 118, 1126, 2738, 33, 84, 377, 155, 1621, 31, 2608, 1147, 40, 2739, 361, 44], [45, 125, 661, 3789, 1615, 1276, 237, 1915, 687, 20, 3790, 44]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ROpYL7gbXQfL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "74c755b6-9357-4ba7-b879-46d3c2a7bb6c"
      },
      "source": [
        "WORD_EMBEDDING_DIM = word_embedding_matrix.shape[1] # 50\n",
        "choices_dim = {\"PoS\":4,\"case_feature\":6,'dependency':4}\n",
        "SENTENCE_EMBEDDING_DIM = sum([choices_dim[choice] for choice in sentence_embedding_choices])\n",
        "\n",
        "EMBEDDING_DIM = WORD_EMBEDDING_DIM + SENTENCE_EMBEDDING_DIM\n",
        "EMBEDDING_DIM"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8ueiOdhxEz9o"
      },
      "source": [
        "# NER Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VkVmZ3jYGq7o"
      },
      "source": [
        "# Reference lab9\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "torch.manual_seed(1)\n",
        "\n",
        "def argmax(vec):\n",
        "    # return the argmax as a python int\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return idx.item()\n",
        "\n",
        "\n",
        "# Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "def log_sum_exp(vec):\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + \\\n",
        "        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "\n",
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,crf = True,attention = 'general',layer = 1,sent_embed_status = True):\n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.crf = crf\n",
        "        self.attention = attention\n",
        "        self.layer = layer\n",
        "        self.sent_embed_status = sent_embed_status\n",
        "\n",
        "        self.word_embeds = nn.Embedding(vocab_size, WORD_EMBEDDING_DIM)\n",
        "        self.fc = nn.Linear(hidden_dim, hidden_dim, bias=False)\n",
        "\n",
        "        \"\"\"Here we use the embedding matrix as the initial weights of nn.Embedding\"\"\"\n",
        "        self.word_embeds.weight.data.copy_(torch.from_numpy(word_embedding_matrix))\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n",
        "                            num_layers=layer, dropout = 0.1, bidirectional=True)\n",
        "\n",
        "        # Maps the output of the LSTM into tag space.\n",
        "        if attention == 'no':\n",
        "          self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n",
        "        else:\n",
        "          self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "\n",
        "        # Matrix of transition parameters.  Entry i,j is the score of\n",
        "        # transitioning *to* i *from* j.\n",
        "        self.transitions = nn.Parameter(\n",
        "            torch.randn(self.tagset_size, self.tagset_size))\n",
        "\n",
        "        # These two statements enforce the constraint that we never transfer\n",
        "        # to the start tag and we never transfer from the stop tag\n",
        "        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "        self.dropout_lstm=nn.Dropout(p=0.5)\n",
        "        self.dropout = nn.Dropout(p=0.3)\n",
        "\n",
        "        self.hidden = self.init_hidden()\n",
        "\n",
        "    def init_hidden(self):\n",
        "        return (torch.randn(2*self.layer, 1, self.hidden_dim // 2).to(device),\n",
        "                torch.randn(2*self.layer, 1, self.hidden_dim // 2).to(device))\n",
        "\n",
        "    def _forward_alg(self, feats):\n",
        "        # Do the forward algorithm to compute the partition function\n",
        "        init_alphas = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        # START_TAG has all of the score.\n",
        "        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "\n",
        "        # Wrap in a variable so that we will get automatic backprop\n",
        "        forward_var = init_alphas\n",
        "\n",
        "        # Iterate through the sentence\n",
        "        for feat in feats:\n",
        "            alphas_t = []  # The forward tensors at this timestep\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # broadcast the emission score: it is the same regardless of\n",
        "                # the previous tag\n",
        "                emit_score = feat[next_tag].view(\n",
        "                    1, -1).expand(1, self.tagset_size)\n",
        "                # the ith entry of trans_score is the score of transitioning to\n",
        "                # next_tag from i\n",
        "                trans_score = self.transitions[next_tag].view(1, -1)\n",
        "                # The ith entry of next_tag_var is the value for the\n",
        "                # edge (i -> next_tag) before we do log-sum-exp\n",
        "                next_tag_var = forward_var + trans_score + emit_score\n",
        "                # The forward variable for this tag is log-sum-exp of all the\n",
        "                # scores.\n",
        "                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n",
        "            forward_var = torch.cat(alphas_t).view(1, -1)\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        alpha = log_sum_exp(terminal_var)\n",
        "        return alpha\n",
        "\n",
        "    def self_attention(self,lstm_out, l, method=None):\n",
        "    \n",
        "        embeds = lstm_out.view(l,-1)\n",
        "        # print(\"emb\",embeds.shape)\n",
        "\n",
        "        k = torch.empty(self.hidden_dim,self.hidden_dim)\n",
        "        #nn.init.uniform_(k,-np.sqrt(self.hidden_dim),np.sqrt(self.hidden_dim))\n",
        "\n",
        "        q = torch.empty(self.hidden_dim,self.hidden_dim)\n",
        "        #nn.init.uniform_(q,-np.sqrt(self.hidden_dim),np.sqrt(self.hidden_dim))\n",
        "\n",
        "        v = torch.empty(self.hidden_dim,self.hidden_dim)\n",
        "        #nn.init.uniform_(v,-np.sqrt(self.hidden_dim),np.sqrt(self.hidden_dim))\n",
        "\n",
        "        torch.nn.init.xavier_normal_(k)\n",
        "        torch.nn.init.xavier_normal_(q)\n",
        "        torch.nn.init.xavier_normal_(v)\n",
        "\n",
        "        \n",
        "        keys = embeds @ k\n",
        "        querys = embeds @ q\n",
        "        values = embeds @ v\n",
        "\n",
        "        method = self.attention\n",
        "\n",
        "        if method == \"dot\":\n",
        "          # For the dot scoring method, no weights or linear layers are involved\n",
        "          attn_scores = querys @ keys.T\n",
        "          attn_scores_softmax = F.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        \n",
        "        elif method == \"general\":\n",
        "          # For general scoring, decoder hidden state is passed through linear layers to introduce a weight matrix\n",
        "          querys = self.fc(querys)\n",
        "          attn_scores = querys @ keys.T\n",
        "          # return encoder_outputs.bmm(out.view(1,-1,1)).squeeze(-1)\n",
        "          attn_scores_softmax = F.softmax(attn_scores, dim=-1)\n",
        "        \n",
        "        elif method == \"scaled\":\n",
        "          # For concat scoring, decoder hidden state and encoder outputs are concatenated first\n",
        "          # out = torch.tanh(self.fc(decoder_hidden+encoder_outputs))\n",
        "          # return out.bmm(self.weight.unsqueeze(-1)).squeeze(-1)\n",
        "          attn_scores = querys @ keys.T\n",
        "          attn_scores_softmax = F.softmax(attn_scores/np.sqrt(self.hidden_dim), dim=-1)\n",
        "\n",
        "      \n",
        "        weighted_values = values[:,None] * attn_scores_softmax.T[:,:,None]\n",
        "        outputs = weighted_values.sum(dim=0)\n",
        "\n",
        "        outputs = outputs.view(-1,1,self.hidden_dim)\n",
        "        #outputs = self.dropout(outputs)\n",
        "\n",
        "        concat_output = torch.cat((outputs, lstm_out), 1)\n",
        "\n",
        "        return concat_output\n",
        "        \n",
        "\n",
        "    def _get_lstm_features(self, sentence):\n",
        "        self.hidden = self.init_hidden()\n",
        "        if self.sent_embed_status:\n",
        "          embeds = self.word_embeds(sentence).view(len(sentence), -1)\n",
        "          sent_embed = torch.tensor(sentence_embeds[tag_to_sent(sentence.tolist())]).float().view(len(sentence), -1)\n",
        "          embeds = torch.cat((embeds,sent_embed),1).view(len(sentence), 1, -1)\n",
        "        else:\n",
        "          embeds = self.word_embeds(sentence).view(len(sentence),1, -1)\n",
        "          \n",
        "        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n",
        "\n",
        "        if self.attention == 'no':\n",
        "          lstm_out = lstm_out.view(-1, self.hidden_dim)\n",
        "        else:\n",
        "          lstm_out = self.self_attention(lstm_out,len(sentence))\n",
        "          lstm_out = lstm_out.view(-1, self.hidden_dim*2)\n",
        "\n",
        "        lstm_out = self.dropout_lstm(lstm_out)\n",
        "        lstm_feats = self.hidden2tag(lstm_out)\n",
        "        return lstm_feats\n",
        "\n",
        "    def _score_sentence(self, feats, tags):\n",
        "        # Gives the score of a provided tag sequence\n",
        "        score = torch.zeros(1).to(device)\n",
        "        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long).to(device), tags])\n",
        "        for i, feat in enumerate(feats):\n",
        "            score = score + \\\n",
        "                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n",
        "        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n",
        "        return score\n",
        "\n",
        "    def _viterbi_decode(self, feats):\n",
        "        backpointers = []\n",
        "\n",
        "        # Initialize the viterbi variables in log space\n",
        "        init_vvars = torch.full((1, self.tagset_size), -10000.).to(device)\n",
        "        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "\n",
        "        # forward_var at step i holds the viterbi variables for step i-1\n",
        "        forward_var = init_vvars\n",
        "        for feat in feats:\n",
        "            bptrs_t = []  # holds the backpointers for this step\n",
        "            viterbivars_t = []  # holds the viterbi variables for this step\n",
        "\n",
        "            for next_tag in range(self.tagset_size):\n",
        "                # next_tag_var[i] holds the viterbi variable for tag i at the\n",
        "                # previous step, plus the score of transitioning\n",
        "                # from tag i to next_tag.\n",
        "                # We don't include the emission scores here because the max\n",
        "                # does not depend on them (we add them in below)\n",
        "                next_tag_var = forward_var + self.transitions[next_tag]\n",
        "                best_tag_id = argmax(next_tag_var)\n",
        "                bptrs_t.append(best_tag_id)\n",
        "                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n",
        "            # Now add in the emission scores, and assign forward_var to the set\n",
        "            # of viterbi variables we just computed\n",
        "            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n",
        "            backpointers.append(bptrs_t)\n",
        "\n",
        "        # Transition to STOP_TAG\n",
        "        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "        best_tag_id = argmax(terminal_var)\n",
        "        path_score = terminal_var[0][best_tag_id]\n",
        "\n",
        "        # Follow the back pointers to decode the best path.\n",
        "        best_path = [best_tag_id]\n",
        "        for bptrs_t in reversed(backpointers):\n",
        "            best_tag_id = bptrs_t[best_tag_id]\n",
        "            best_path.append(best_tag_id)\n",
        "        # Pop off the start tag (we dont want to return that to the caller)\n",
        "        start = best_path.pop()\n",
        "        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n",
        "        best_path.reverse()\n",
        "        return path_score, best_path\n",
        "\n",
        "    def neg_log_likelihood(self, sentence, tags):\n",
        "        feats = self._get_lstm_features(sentence)\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "\n",
        "    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n",
        "        # Get the emission scores from the BiLSTM\n",
        "        lstm_feats = self._get_lstm_features(sentence)\n",
        "        \n",
        "        if not self.crf:\n",
        "          return 0,torch.argmax(lstm_feats,dim=1)\n",
        "\n",
        "        score, tag_seq = self._viterbi_decode(lstm_feats)\n",
        "        return score, tag_seq"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Vd1wvX_Hywt"
      },
      "source": [
        "# Reference lab9\n",
        "import numpy as np\n",
        "import itertools\n",
        "\n",
        "def tag_to_sent(sentence):\n",
        "  sent = []\n",
        "  for idx in sentence:\n",
        "    sent.append(ix_to_word[idx])\n",
        "  return ' '.join(sent)\n",
        "\n",
        "def cal_acc(model, input_index, output_index):\n",
        "  predicted = []\n",
        "  for sentence in input_index:\n",
        "    score, tag_seq = model.forward(torch.tensor(sentence).to(device))\n",
        "    predicted.append(tag_seq)\n",
        "  predicted =  list(itertools.chain.from_iterable(predicted))\n",
        "  ground_truth = list(itertools.chain.from_iterable(output_index))\n",
        "  correct = 0\n",
        "  for i in range(0,len(predicted)):\n",
        "    if predicted[i] == ground_truth[i]:\n",
        "      correct += 1\n",
        "  return predicted,ground_truth,((correct/len(predicted)))"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMk-9zk9RwRq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 494
        },
        "outputId": "6c580945-55dd-4a77-84cc-888e416c0b76"
      },
      "source": [
        "# IMPORTANT TIPS:\n",
        "# Try this if running NER models shows a \n",
        "\n",
        "# RuntimeError: PyTorch was compiled without NumPy support\n",
        "\n",
        "!pip install numpy==1.16.4\n",
        "!pip install torch==1.6.0"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting numpy==1.16.4\n",
            "  Using cached https://files.pythonhosted.org/packages/fc/d1/45be1144b03b6b1e24f9a924f23f66b4ad030d834ad31fb9e5581bd328af/numpy-1.16.4-cp37-cp37m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: xarray 0.18.2 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchvision 0.9.1+cu101 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: torchtext 0.9.1 has requirement torch==1.8.1, but you'll have torch 1.6.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement h5py~=3.1.0, but you'll have h5py 3.2.1 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement numpy~=1.19.2, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: tensorflow 2.5.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyerfa 2.0.0 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: pyarrow 3.0.0 has requirement numpy>=1.16.6, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: kapre 0.3.5 has requirement numpy>=1.18.5, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: google-colab 1.0.0 has requirement six~=1.15.0, but you'll have six 1.16.0 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: datascience 0.10.6 has requirement folium==0.2.1, but you'll have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: astropy 4.2.1 has requirement numpy>=1.17, but you'll have numpy 1.16.4 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Installing collected packages: numpy\n",
            "  Found existing installation: numpy 1.19.5\n",
            "    Uninstalling numpy-1.19.5:\n",
            "      Successfully uninstalled numpy-1.19.5\n",
            "Successfully installed numpy-1.16.4\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.7/dist-packages (1.6.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (0.16.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.6.0) (1.16.4)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qdo_NocUH0Xk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3618771f-5aad-4fc4-ed88-2e8301bb837b"
      },
      "source": [
        "import torch\n",
        "\n",
        "device = torch.device(\"cpu\")\n",
        "HIDDEN_DIM = 128\n",
        "\n",
        "model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM).to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ymWg1_BqH1W2"
      },
      "source": [
        "\"\"\"Each epoch will take about 1-2 minutes\"\"\"\n",
        "\n",
        "import datetime\n",
        "# Early stopping\n",
        "def start_training(model):\n",
        "  the_last_loss = 100\n",
        "  patience = 2\n",
        "  trigger_times = 0\n",
        "  best = model\n",
        "\n",
        "  for epoch in range(15):  \n",
        "      time1 = datetime.datetime.now()\n",
        "      train_loss = 0\n",
        "\n",
        "      model.train()\n",
        "      for i, idxs in enumerate(train_input_index):\n",
        "          tags_index = train_output_index[i]\n",
        "\n",
        "          # Step 1. Remember that Pytorch accumulates gradients.\n",
        "          # We need to clear them out before each instance\n",
        "          model.zero_grad()\n",
        "\n",
        "          # Step 2. Get our inputs ready for the network, that is,\n",
        "          # turn them into Tensors of word indices.\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "\n",
        "          # Step 3. Run our forward pass.\n",
        "          loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "\n",
        "          # Step 4. Compute the loss, gradients, and update the parameters by\n",
        "          # calling optimizer.step()\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "\n",
        "          train_loss+=loss.item()\n",
        "\n",
        "      model.eval()\n",
        "      # Call the cal_acc functions you implemented as required\n",
        "      _, _, train_acc = cal_acc(model,train_input_index,train_output_index)\n",
        "      _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "\n",
        "      val_loss = 0\n",
        "      for i, idxs in enumerate(val_input_index):\n",
        "          tags_index = val_output_index[i]\n",
        "          sentence_in = torch.tensor(idxs, dtype=torch.long).to(device)\n",
        "          targets = torch.tensor(tags_index, dtype=torch.long).to(device)\n",
        "          loss = model.neg_log_likelihood(sentence_in, targets)\n",
        "          val_loss+=loss.item()\n",
        "      time2 = datetime.datetime.now()\n",
        "\n",
        "      # # Early stopping\n",
        "      if val_loss > the_last_loss:\n",
        "          trigger_times += 1\n",
        "          # print('trigger times:', trigger_times)\n",
        "          if trigger_times >= patience:\n",
        "              print('Early stopping!\\nProceed to prediciton process.')\n",
        "              break\n",
        "      else:\n",
        "          # print('trigger times: 0')\n",
        "          trigger_times = 0\n",
        "          best = model\n",
        "      the_last_loss = val_loss\n",
        "\n",
        "      print(\"Epoch:%d, Training loss: %.2f, train acc: %.4f, val loss: %.2f, val acc: %.4f, time: %.2fs, Early Stopping trigger times: %d\" %(epoch+1, train_loss,train_acc, val_loss, val_acc, (time2-time1).total_seconds(), trigger_times))\n",
        "\n",
        "  # use the model that did not show increase in loss\n",
        "  model = best\n",
        "  # The log below is the sample output for this section"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IwZ0lRIcm6hY"
      },
      "source": [
        "start_training(model)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcxLdZhTS8IO"
      },
      "source": [
        "predicted = []\n",
        "for sentence in test_uncased_input_index:\n",
        "  score, tag_seq = model.forward(torch.tensor(sentence).to(device))\n",
        "  #tag_seq = [int(x)-2 for x in tag_seq]\n",
        "  tag_seq = [ix_to_tag[int(x)] for x in tag_seq]\n",
        "  predicted.append(tag_seq)\n",
        "predicted =  list(itertools.chain.from_iterable(predicted))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uby1EbZHVwah"
      },
      "source": [
        "print(\"There are a total of {} tags\".format(len(predicted)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ToF0Wsh5JU5K"
      },
      "source": [
        "To submission"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bjieKtBbH39d"
      },
      "source": [
        "# import pandas as pd\n",
        "# df = pd.read_csv('submisson-pos-glove-dp--wiki50.csv', index_col=0)\n",
        "\n",
        "ids = [i for i in range(len(predicted))]\n",
        "df = pd.DataFrame(\n",
        "    {\n",
        "        'Id':ids,\n",
        "     'Predicted': predicted\n",
        "    })\n",
        "\n",
        "df.to_csv('submission-bestmodel.csv', index=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuYX0VMgOW8p"
      },
      "source": [
        "##Testing and Evaluation - Setup\n",
        "\n",
        "**DO NOT run the code under setup secetion, those are just evidence that showing the results are come form our model** \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4xJOCWi_vuI"
      },
      "source": [
        "**1. Baseline model**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55qYvggK_1Kk",
        "outputId": "8756b8b7-63fc-411d-bc34-5edc187b4ec3"
      },
      "source": [
        "# import gensim.downloader as api\n",
        "# word_emb_model_base = api.load(\"glove-twitter-100\")\n",
        "\n",
        "# # word to vector embedding example\n",
        "# # word_emb_model4['iraq']"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[==================================================] 100.0% 387.1/387.1MB downloaded\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZER5yxlmAHou",
        "outputId": "5aaaf260-1d04-4d02-b4d0-fb3701cda03a"
      },
      "source": [
        "# import torch\n",
        "\n",
        "# word_embedding_matrix = []\n",
        "# for i in range(len(word_list)):\n",
        "#     word = word_list[i]\n",
        "\n",
        "#     if word.lower() in word_emb_model_base:\n",
        "#       v = word_emb_model_base.wv[word.lower()]\n",
        "#       w1 = torch.tensor([v.tolist()]).float()\n",
        "#     # word embedding for OOV\n",
        "#     else:\n",
        "#       w1 = torch.tensor([[0]*100]).float()\n",
        "\n",
        "#     word_embedding_matrix.append(w1.tolist()[0])\n",
        "\n",
        "\n",
        "# word_embedding_matrix = np.array(word_embedding_matrix)\n",
        "# word_embedding_matrix.shape"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:8: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4291, 50)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 112
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yRViPoxRAZX6",
        "outputId": "5b9f79e3-8b6b-4125-caf6-77d338169f39"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='no',sent_embed_status = False,crf=True,layer=1).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'baseline-model.pt')"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 15635.23, train acc: 0.7620, val loss: 4550.30, val acc: 0.7026, time: 119.71s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8860.65, train acc: 0.8094, val loss: 3534.04, val acc: 0.7523, time: 120.27s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 6922.89, train acc: 0.8340, val loss: 3070.07, val acc: 0.7707, time: 120.12s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 5886.88, train acc: 0.8538, val loss: 2730.00, val acc: 0.7827, time: 120.17s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5101.98, train acc: 0.8546, val loss: 2660.27, val acc: 0.7858, time: 120.06s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4513.29, train acc: 0.8745, val loss: 2468.29, val acc: 0.7961, time: 119.57s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4138.76, train acc: 0.8845, val loss: 2375.49, val acc: 0.8033, time: 120.03s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 3641.68, train acc: 0.9018, val loss: 2310.21, val acc: 0.8104, time: 119.38s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3389.21, train acc: 0.9028, val loss: 2372.64, val acc: 0.8063, time: 120.19s, Early Stopping trigger times: 1\n",
            "Epoch:10, Training loss: 3043.16, train acc: 0.9158, val loss: 2250.11, val acc: 0.8173, time: 120.20s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 2869.54, train acc: 0.9255, val loss: 2255.52, val acc: 0.8173, time: 120.27s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ggwg4juXIDjq"
      },
      "source": [
        "**2. Ablation Study - different input embedding model**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C6f7N3LhIhUV"
      },
      "source": [
        "~~~\n",
        "Base model -glove + 1 layer + crf\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kpCoHgDIsYD"
      },
      "source": [
        "# # Change this line to choose your desired word-level embedding choices\n",
        "# sentence_embedding_choices = []\n",
        "# sentence_embeds = get_sentence_embedding(all_uncased_data, sentence_embedding_choices)\n",
        "# WORD_EMBEDDING_DIM = word_embedding_matrix.shape[1] # 50\n",
        "# choices_dim = {\"PoS\":4,\"case_feature\":6,'dependency':4}\n",
        "# SENTENCE_EMBEDDING_DIM = sum([choices_dim[choice] for choice in sentence_embedding_choices])\n",
        "\n",
        "# EMBEDDING_DIM = WORD_EMBEDDING_DIM + SENTENCE_EMBEDDING_DIM\n",
        "# EMBEDDING_DIM = 50"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3oSedQtOKMgt",
        "outputId": "02905441-61b5-4b1b-b812-807834fba323"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled',sent_embed_status = False,crf=True).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-glove.pt')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14747.81, train acc: 0.7401, val loss: 4795.25, val acc: 0.6799, time: 123.16s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 9425.27, train acc: 0.7848, val loss: 3543.07, val acc: 0.7205, time: 122.07s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7480.08, train acc: 0.7982, val loss: 3071.07, val acc: 0.7438, time: 122.95s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6393.17, train acc: 0.8213, val loss: 2716.78, val acc: 0.7620, time: 122.72s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5686.42, train acc: 0.8364, val loss: 2471.11, val acc: 0.7767, time: 122.65s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4954.25, train acc: 0.8523, val loss: 2243.60, val acc: 0.7862, time: 123.12s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4518.76, train acc: 0.8646, val loss: 2122.03, val acc: 0.7992, time: 123.79s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4184.88, train acc: 0.8670, val loss: 2152.64, val acc: 0.8009, time: 123.38s, Early Stopping trigger times: 1\n",
            "Epoch:9, Training loss: 3899.78, train acc: 0.8780, val loss: 2039.76, val acc: 0.8121, time: 123.10s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3624.83, train acc: 0.8700, val loss: 2218.52, val acc: 0.8016, time: 123.76s, Early Stopping trigger times: 1\n",
            "Epoch:11, Training loss: 3414.40, train acc: 0.8912, val loss: 2036.03, val acc: 0.8163, time: 124.31s, Early Stopping trigger times: 0\n",
            "Epoch:12, Training loss: 3210.04, train acc: 0.8871, val loss: 2142.70, val acc: 0.8128, time: 123.48s, Early Stopping trigger times: 1\n",
            "Epoch:13, Training loss: 3029.82, train acc: 0.9036, val loss: 2014.90, val acc: 0.8247, time: 123.18s, Early Stopping trigger times: 0\n",
            "Epoch:14, Training loss: 2940.76, train acc: 0.9090, val loss: 1951.44, val acc: 0.8193, time: 123.74s, Early Stopping trigger times: 0\n",
            "Epoch:15, Training loss: 2722.98, train acc: 0.9038, val loss: 2092.73, val acc: 0.8223, time: 124.10s, Early Stopping trigger times: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GpfINXaWID2N"
      },
      "source": [
        "~~~\n",
        "Syntactic Textual Feature Embedding:  POS tag information\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5O1fxBOEIs5b",
        "outputId": "550e5903-76cb-4d66-a571-8cb7228b3cb9"
      },
      "source": [
        "# sentence_embedding_choices = [\"PoS\"]\n",
        "# sentence_embeds = get_sentence_embedding(all_uncased_data, sentence_embedding_choices)\n",
        "# WORD_EMBEDDING_DIM = word_embedding_matrix.shape[1] # 50\n",
        "# choices_dim = {\"PoS\":4,\"case_feature\":6,'dependency':4}\n",
        "# SENTENCE_EMBEDDING_DIM = sum([choices_dim[choice] for choice in sentence_embedding_choices])\n",
        "\n",
        "# EMBEDDING_DIM = WORD_EMBEDDING_DIM + SENTENCE_EMBEDDING_DIM\n",
        "# EMBEDDING_DIM"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "54"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 113
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jktfSc9nKf97",
        "outputId": "e3f3d8cb-e9da-47f5-cb54-dabdd4cfc85c"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='general',sent_embed_status = True,crf=True).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-glove-pos.pt')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 15527.45, train acc: 0.7433, val loss: 4830.98, val acc: 0.6905, time: 122.15s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 9293.35, train acc: 0.7902, val loss: 3669.94, val acc: 0.7376, time: 121.64s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7306.16, train acc: 0.8178, val loss: 3008.75, val acc: 0.7665, time: 123.27s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6280.31, train acc: 0.8241, val loss: 2832.48, val acc: 0.7633, time: 121.73s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5418.59, train acc: 0.8445, val loss: 2451.62, val acc: 0.7841, time: 122.30s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4814.53, train acc: 0.8613, val loss: 2286.99, val acc: 0.7927, time: 123.12s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4419.86, train acc: 0.8670, val loss: 2182.21, val acc: 0.7987, time: 122.34s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4058.12, train acc: 0.8830, val loss: 2002.31, val acc: 0.8201, time: 122.50s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3675.38, train acc: 0.8935, val loss: 1924.37, val acc: 0.8186, time: 122.81s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3507.92, train acc: 0.9009, val loss: 1936.19, val acc: 0.8255, time: 122.84s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UJw_xiA-ItYv"
      },
      "source": [
        "~~~\n",
        "Semantic Textual Feature Embedding: glove-wiki-gigaword-50\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lquIisHeI4ea"
      },
      "source": [
        "~~~\n",
        "Domain Feature Embedding: case features\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h9RB_2UnI3bV",
        "outputId": "119e4d1a-80a8-4193-bac3-d4ecf8bccc8e"
      },
      "source": [
        "# sentence_embedding_choices = [\"PoS\",\"case_feature\"]\n",
        "# sentence_embeds = get_sentence_embedding(all_uncased_data, sentence_embedding_choices)\n",
        "# WORD_EMBEDDING_DIM = word_embedding_matrix.shape[1] # 50\n",
        "# choices_dim = {\"PoS\":4,\"case_feature\":6,'dependency':4}\n",
        "# SENTENCE_EMBEDDING_DIM = sum([choices_dim[choice] for choice in sentence_embedding_choices])\n",
        "\n",
        "# EMBEDDING_DIM = WORD_EMBEDDING_DIM + SENTENCE_EMBEDDING_DIM\n",
        "# EMBEDDING_DIM"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pyYRBmhuKqHZ",
        "outputId": "9d5ee93d-53ec-4182-a6fd-703dd4f5a3ca"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled',sent_embed_status = True,crf=True).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-glove-pos-case.pt')"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 15073.15, train acc: 0.7712, val loss: 4474.64, val acc: 0.7153, time: 123.13s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8799.65, train acc: 0.8042, val loss: 3426.09, val acc: 0.7581, time: 122.83s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7033.62, train acc: 0.8218, val loss: 3002.46, val acc: 0.7652, time: 123.81s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 5999.13, train acc: 0.8304, val loss: 2767.18, val acc: 0.7737, time: 122.18s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5263.33, train acc: 0.8477, val loss: 2436.75, val acc: 0.7968, time: 122.65s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4828.98, train acc: 0.8561, val loss: 2311.63, val acc: 0.7988, time: 122.41s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4415.15, train acc: 0.8623, val loss: 2158.07, val acc: 0.8113, time: 122.95s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4051.91, train acc: 0.8768, val loss: 2102.01, val acc: 0.8219, time: 123.02s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3672.41, train acc: 0.8778, val loss: 2101.42, val acc: 0.8169, time: 122.40s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3513.95, train acc: 0.8908, val loss: 1957.28, val acc: 0.8245, time: 123.13s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3300.44, train acc: 0.8898, val loss: 2053.44, val acc: 0.8238, time: 124.36s, Early Stopping trigger times: 1\n",
            "Epoch:12, Training loss: 3156.86, train acc: 0.9064, val loss: 1843.09, val acc: 0.8342, time: 124.17s, Early Stopping trigger times: 0\n",
            "Epoch:13, Training loss: 2973.54, train acc: 0.9101, val loss: 1878.52, val acc: 0.8286, time: 122.76s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uyRlqBfVh005"
      },
      "source": [
        "**3. Ablation Study - different attention strategy**\n",
        "\n",
        "```\n",
        "Base model - glove + case features + pos + 1 layer + crf\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FCAZLSqOh3BX"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='no').to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-no-attention.pt')"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WBPdjBL8paDQ"
      },
      "source": [
        "~~~\n",
        "Dot-product\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pC1aslApnGXJ",
        "outputId": "cf8f5a2c-49a5-4486-b7d0-e9369a73bbaa"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='dot').to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-dot-product.pt')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14709.39, train acc: 0.7714, val loss: 4406.71, val acc: 0.7265, time: 120.75s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8709.91, train acc: 0.7994, val loss: 3534.63, val acc: 0.7536, time: 120.73s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7113.20, train acc: 0.8212, val loss: 3103.72, val acc: 0.7704, time: 120.48s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6073.18, train acc: 0.8364, val loss: 2788.99, val acc: 0.7825, time: 120.90s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5423.05, train acc: 0.8498, val loss: 2474.28, val acc: 0.7942, time: 121.25s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4861.61, train acc: 0.8583, val loss: 2276.70, val acc: 0.8050, time: 121.14s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4430.91, train acc: 0.8683, val loss: 2145.25, val acc: 0.8167, time: 121.35s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4092.48, train acc: 0.8766, val loss: 2106.23, val acc: 0.8180, time: 120.63s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3866.41, train acc: 0.8836, val loss: 2053.09, val acc: 0.8201, time: 119.74s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3495.97, train acc: 0.8931, val loss: 1953.88, val acc: 0.8210, time: 120.17s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3344.94, train acc: 0.8972, val loss: 1961.13, val acc: 0.8277, time: 120.32s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3j7_IQIRpkGt"
      },
      "source": [
        "~~~\n",
        "Scaled dot-product\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VkV8-cYHpm5n",
        "outputId": "716c87ef-f7f8-40d8-fc6c-7371de04e241"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled').to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-scaled-dot-product.pt')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14982.51, train acc: 0.7638, val loss: 4509.81, val acc: 0.7114, time: 120.45s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8784.68, train acc: 0.8005, val loss: 3532.51, val acc: 0.7441, time: 119.52s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 6980.21, train acc: 0.8168, val loss: 2994.10, val acc: 0.7691, time: 119.54s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 5872.61, train acc: 0.8430, val loss: 2633.98, val acc: 0.7838, time: 120.02s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5280.14, train acc: 0.8571, val loss: 2336.38, val acc: 0.8033, time: 120.52s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4811.48, train acc: 0.8640, val loss: 2212.96, val acc: 0.8063, time: 120.54s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4326.30, train acc: 0.8726, val loss: 2087.79, val acc: 0.8122, time: 120.37s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 3898.82, train acc: 0.8804, val loss: 2073.17, val acc: 0.8096, time: 120.20s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3755.09, train acc: 0.8851, val loss: 1973.01, val acc: 0.8227, time: 127.66s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3503.23, train acc: 0.8914, val loss: 1853.14, val acc: 0.8251, time: 124.59s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3251.48, train acc: 0.8988, val loss: 1958.55, val acc: 0.8262, time: 125.42s, Early Stopping trigger times: 1\n",
            "Epoch:12, Training loss: 3037.14, train acc: 0.9092, val loss: 1866.42, val acc: 0.8353, time: 126.32s, Early Stopping trigger times: 0\n",
            "Epoch:13, Training loss: 2897.57, train acc: 0.9110, val loss: 1871.98, val acc: 0.8342, time: 125.22s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poYulC3lp0KC"
      },
      "source": [
        "~~~\n",
        "General\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T50k13Frpy3B",
        "outputId": "e39b92eb-67a2-4bf4-b9ed-52d7623c552a"
      },
      "source": [
        "# device = torch.device(\"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='general').to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-general.pt')"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14591.74, train acc: 0.7662, val loss: 4407.14, val acc: 0.7138, time: 120.53s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8879.62, train acc: 0.7971, val loss: 3530.93, val acc: 0.7492, time: 121.71s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7212.03, train acc: 0.8217, val loss: 2988.05, val acc: 0.7670, time: 121.47s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6206.05, train acc: 0.8366, val loss: 2623.92, val acc: 0.7832, time: 120.49s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5489.73, train acc: 0.8370, val loss: 2553.20, val acc: 0.7841, time: 120.80s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4924.69, train acc: 0.8576, val loss: 2249.89, val acc: 0.8033, time: 120.27s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4500.39, train acc: 0.8678, val loss: 2183.46, val acc: 0.8072, time: 120.22s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4162.30, train acc: 0.8735, val loss: 2144.17, val acc: 0.8085, time: 119.74s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3850.00, train acc: 0.8809, val loss: 2034.18, val acc: 0.8178, time: 120.38s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3678.98, train acc: 0.8913, val loss: 1948.03, val acc: 0.8253, time: 121.09s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3383.87, train acc: 0.8968, val loss: 1953.20, val acc: 0.8264, time: 120.30s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9keg3PVMp87D"
      },
      "source": [
        "**4. Ablation Study - different Stacked layer or # of encoder/decoder strategy**\n",
        "\n",
        "~~~\n",
        "Base model: glove + pos + case features + scaled dot-product + 1-layer + crf\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R5bOLxyNp7Q-",
        "outputId": "0ebff235-ff4c-42fc-e7d6-1814ddbd6a2f"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled', layer = 1).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-layer&crf.pt')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14726.83, train acc: 0.7620, val loss: 4203.32, val acc: 0.7099, time: 121.27s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8709.66, train acc: 0.8020, val loss: 3280.22, val acc: 0.7568, time: 120.55s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 6979.37, train acc: 0.8191, val loss: 2848.00, val acc: 0.7737, time: 120.30s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 5955.63, train acc: 0.8367, val loss: 2608.30, val acc: 0.7810, time: 120.26s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5268.82, train acc: 0.8514, val loss: 2299.05, val acc: 0.7955, time: 120.36s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4776.85, train acc: 0.8626, val loss: 2128.08, val acc: 0.8083, time: 120.89s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4332.62, train acc: 0.8714, val loss: 2089.16, val acc: 0.8119, time: 120.54s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4033.14, train acc: 0.8748, val loss: 2104.21, val acc: 0.8109, time: 120.66s, Early Stopping trigger times: 1\n",
            "Epoch:9, Training loss: 3673.16, train acc: 0.8864, val loss: 1992.70, val acc: 0.8227, time: 120.41s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3488.18, train acc: 0.8956, val loss: 1903.40, val acc: 0.8294, time: 120.41s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3276.23, train acc: 0.9004, val loss: 1857.37, val acc: 0.8266, time: 120.85s, Early Stopping trigger times: 0\n",
            "Epoch:12, Training loss: 3110.39, train acc: 0.8997, val loss: 1998.21, val acc: 0.8229, time: 120.62s, Early Stopping trigger times: 1\n",
            "Early stopping!\n",
            "Proceed to prediciton process.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SF-3TwzqScX"
      },
      "source": [
        "~~~\n",
        "2-layer\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0VuYw4SMqRe-",
        "outputId": "f395b910-e775-4599-d2bc-0cd2e1af99bd"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='general', layer = 2).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-2-layer')"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 16146.15, train acc: 0.7273, val loss: 5279.17, val acc: 0.6738, time: 132.91s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 10234.20, train acc: 0.7836, val loss: 3803.95, val acc: 0.7410, time: 132.35s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 8007.77, train acc: 0.8035, val loss: 3374.34, val acc: 0.7518, time: 132.64s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6683.39, train acc: 0.8242, val loss: 2958.45, val acc: 0.7773, time: 132.64s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5784.23, train acc: 0.8450, val loss: 2535.71, val acc: 0.7908, time: 132.88s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 5143.91, train acc: 0.8562, val loss: 2404.02, val acc: 0.8007, time: 132.93s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4591.42, train acc: 0.8678, val loss: 2240.57, val acc: 0.8082, time: 132.43s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4164.64, train acc: 0.8722, val loss: 2115.14, val acc: 0.8212, time: 133.38s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 3858.34, train acc: 0.8818, val loss: 2109.18, val acc: 0.8193, time: 133.05s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3492.60, train acc: 0.8926, val loss: 2020.60, val acc: 0.8277, time: 133.47s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3243.62, train acc: 0.8975, val loss: 2001.27, val acc: 0.8258, time: 133.82s, Early Stopping trigger times: 0\n",
            "Epoch:12, Training loss: 2967.72, train acc: 0.8936, val loss: 2093.03, val acc: 0.8188, time: 133.55s, Early Stopping trigger times: 1\n",
            "Epoch:13, Training loss: 2819.65, train acc: 0.9060, val loss: 2017.31, val acc: 0.8284, time: 132.98s, Early Stopping trigger times: 0\n",
            "Epoch:14, Training loss: 2658.98, train acc: 0.9064, val loss: 2103.65, val acc: 0.8251, time: 133.41s, Early Stopping trigger times: 1\n",
            "Epoch:15, Training loss: 2464.16, train acc: 0.9161, val loss: 2083.30, val acc: 0.8253, time: 133.58s, Early Stopping trigger times: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HBtmPVn4qR8p"
      },
      "source": [
        "~~~\n",
        "3-layer\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ND4ABUo3qZ-j",
        "outputId": "18f2c900-4a12-464e-af7d-5547b5ac16e8"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled', layer = 3).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-3-layer')"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 17996.30, train acc: 0.7008, val loss: 6071.56, val acc: 0.6479, time: 142.34s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 12688.39, train acc: 0.7459, val loss: 4889.34, val acc: 0.6885, time: 143.73s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 9727.87, train acc: 0.7705, val loss: 3958.76, val acc: 0.7216, time: 145.60s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 8150.32, train acc: 0.8014, val loss: 3423.74, val acc: 0.7516, time: 145.25s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 6950.12, train acc: 0.8116, val loss: 3169.41, val acc: 0.7575, time: 146.09s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 6115.05, train acc: 0.8316, val loss: 2868.89, val acc: 0.7758, time: 144.25s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 5512.18, train acc: 0.8397, val loss: 2651.63, val acc: 0.7828, time: 144.78s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 5078.23, train acc: 0.8515, val loss: 2450.85, val acc: 0.7985, time: 146.45s, Early Stopping trigger times: 0\n",
            "Epoch:9, Training loss: 4580.52, train acc: 0.8673, val loss: 2287.13, val acc: 0.8039, time: 145.80s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 4196.22, train acc: 0.8718, val loss: 2262.19, val acc: 0.8009, time: 144.83s, Early Stopping trigger times: 0\n",
            "Epoch:11, Training loss: 3816.50, train acc: 0.8859, val loss: 2154.67, val acc: 0.8137, time: 144.66s, Early Stopping trigger times: 0\n",
            "Epoch:12, Training loss: 3563.34, train acc: 0.8867, val loss: 2144.47, val acc: 0.8132, time: 145.53s, Early Stopping trigger times: 0\n",
            "Epoch:13, Training loss: 3244.05, train acc: 0.8944, val loss: 2129.90, val acc: 0.8128, time: 146.09s, Early Stopping trigger times: 0\n",
            "Epoch:14, Training loss: 2995.95, train acc: 0.9058, val loss: 2034.57, val acc: 0.8258, time: 144.99s, Early Stopping trigger times: 0\n",
            "Epoch:15, Training loss: 2915.60, train acc: 0.9064, val loss: 2213.14, val acc: 0.8284, time: 144.39s, Early Stopping trigger times: 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB82BWsxqctX"
      },
      "source": [
        "**5. Ablation Study - with/without CRF**\n",
        "\n",
        "~~~\n",
        "Base model: glove + pos + case features + scaled + 1-layer + crf\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kSZXhJWsqb_2"
      },
      "source": [
        "# device = torch.device(\"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='scaled', layer = 1,crf = True).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-with-crf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4_doTX6qnu8"
      },
      "source": [
        "~~~\n",
        "With out crf\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1ms0l41qm8L",
        "outputId": "ff15bae1-8ed8-4796-8051-12c761a74661"
      },
      "source": [
        "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "# HIDDEN_DIM = 128\n",
        "# model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM,attention='general', layer = 1,crf = False).to(device)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n",
        "\n",
        "# start_training(model)\n",
        "# torch.save(model,'basemodel-without-crf')"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:60: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
            "  \"num_layers={}\".format(dropout, num_layers))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch:1, Training loss: 14754.39, train acc: 0.7497, val loss: 4565.45, val acc: 0.7039, time: 107.68s, Early Stopping trigger times: 1\n",
            "Epoch:2, Training loss: 8826.29, train acc: 0.7654, val loss: 3467.57, val acc: 0.7131, time: 105.69s, Early Stopping trigger times: 0\n",
            "Epoch:3, Training loss: 7129.69, train acc: 0.7777, val loss: 3033.94, val acc: 0.7268, time: 106.52s, Early Stopping trigger times: 0\n",
            "Epoch:4, Training loss: 6123.78, train acc: 0.7848, val loss: 2668.77, val acc: 0.7367, time: 105.01s, Early Stopping trigger times: 0\n",
            "Epoch:5, Training loss: 5367.21, train acc: 0.7954, val loss: 2459.93, val acc: 0.7428, time: 104.77s, Early Stopping trigger times: 0\n",
            "Epoch:6, Training loss: 4890.44, train acc: 0.7994, val loss: 2301.04, val acc: 0.7475, time: 105.20s, Early Stopping trigger times: 0\n",
            "Epoch:7, Training loss: 4356.73, train acc: 0.8028, val loss: 2061.03, val acc: 0.7493, time: 106.10s, Early Stopping trigger times: 0\n",
            "Epoch:8, Training loss: 4038.99, train acc: 0.8020, val loss: 2086.98, val acc: 0.7460, time: 104.92s, Early Stopping trigger times: 1\n",
            "Epoch:9, Training loss: 3778.88, train acc: 0.8074, val loss: 1950.19, val acc: 0.7464, time: 105.38s, Early Stopping trigger times: 0\n",
            "Epoch:10, Training loss: 3452.28, train acc: 0.8056, val loss: 2042.84, val acc: 0.7499, time: 105.35s, Early Stopping trigger times: 1\n",
            "Epoch:11, Training loss: 3265.71, train acc: 0.8174, val loss: 2023.22, val acc: 0.7546, time: 105.57s, Early Stopping trigger times: 0\n",
            "Epoch:12, Training loss: 3090.40, train acc: 0.8111, val loss: 1997.72, val acc: 0.7527, time: 106.08s, Early Stopping trigger times: 0\n",
            "Epoch:13, Training loss: 2877.79, train acc: 0.8252, val loss: 1994.49, val acc: 0.7551, time: 106.03s, Early Stopping trigger times: 0\n",
            "Epoch:14, Training loss: 2709.43, train acc: 0.8252, val loss: 2151.18, val acc: 0.7560, time: 105.79s, Early Stopping trigger times: 1\n",
            "Epoch:15, Training loss: 2639.64, train acc: 0.8134, val loss: 2074.51, val acc: 0.7451, time: 105.95s, Early Stopping trigger times: 0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNOSAu5hqueI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9K995xFQoef"
      },
      "source": [
        "##Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vDFxwWmiRCBd"
      },
      "source": [
        "**1. Performance Comparison**\n",
        "\n",
        "~~~\n",
        "Baseline model:\n",
        "\n",
        "Model: Bi-lstm+crf\n",
        " \n",
        "Input embedding dimension: word embedding (glove-twitter) 100\n",
        " \n",
        "Epochs: 15 with early stopping\n",
        " \n",
        "Learing rate: 0.01\n",
        " \n",
        "Weight decay: 1e-4\n",
        " \n",
        "Optimiser: SGD\n",
        " \n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 50\n",
        "~~~\n",
        "\n",
        "~~~\n",
        "Best model:\n",
        "\n",
        "Model: Bi-lstm + crf + scaled dot-product attention + 1 layer\n",
        " \n",
        "Input embedding dimension: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6\n",
        " \n",
        "Epochs: 15 with early stopping\n",
        " \n",
        "Learing rate: 0.01\n",
        " \n",
        "Weight decay: 1e-4\n",
        " \n",
        "Optimiser: SGD\n",
        " \n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 128\n",
        "~~~\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nw-84qkOJtk4",
        "outputId": "1ca1d747-e150-4963-c81c-6f8663af16fc"
      },
      "source": [
        "!pip install tabulate\n",
        "from tabulate import tabulate\n",
        "\n",
        "def get_accuracy(model_name,is_gpu=False):\n",
        "  # Load Model\n",
        "  model = torch.load(model_name)\n",
        "  model.eval()\n",
        "\n",
        "  from sklearn.metrics import classification_report\n",
        "  #print(classification_report(ground_truth, predicted))\n",
        "  \n",
        "  _, _, val_acc = cal_acc(model,val_input_index,val_output_index)\n",
        "  print(\"val acc: %.4f\"%val_acc)\n",
        "\n",
        "  return val_acc"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.7/dist-packages (0.8.9)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wHWKDgFQztn",
        "outputId": "eac85318-3788-4256-a28c-0b7bf9ae5bf8"
      },
      "source": [
        "title = ['Models','Accuracy']\n",
        "acc_base = get_accuracy('baseline-model.pt')\n",
        "acc_2 = get_accuracy('best-model.pt')\n",
        "\n",
        "table = [title,\n",
        "         ['Base model (lab 9)',acc_base],\n",
        "          ['Our model:',acc_2]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 0.8201\n",
            "val acc: 0.8377\n",
            "\n",
            " Models                Accuracy \n",
            "\n",
            " Base model (lab 9)    0.82006  \n",
            "\n",
            " Our model:            0.837737 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7X_HbVyRGDH"
      },
      "source": [
        "**2. Ablation Study - different input embedding model**\n",
        "\n",
        "~~~\n",
        "Model: Bi-lstm+crf\n",
        "\n",
        "Base model: word embedding (glove-wiki) 50 + scaled dot-product + crf + 1 layer\n",
        "\n",
        "Input embedding dimension: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6\n",
        "\n",
        "Epochs: 15 with early stopping\n",
        "\n",
        "Learing rate: 0.01\n",
        "\n",
        "Weight decay: 1e-4\n",
        "\n",
        "Optimiser: SGD\n",
        "\n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 128\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VpzTmhOXRIeL",
        "outputId": "b4693155-ce05-4ad8-c766-6164181ed374"
      },
      "source": [
        "\n",
        "title = ['Models','Accuracy']\n",
        "acc_base = get_accuracy('basemodel-glove.pt')\n",
        "acc_1 = get_accuracy('basemodel-glove-pos.pt')\n",
        "acc_2 = get_accuracy('best-model.pt')\n",
        "\n",
        "table = [title,\n",
        "         ['Base model (glove-wiki)',acc_base],\n",
        "          ['Syntactic Textual Feature Embedding: PoS tag information',acc_1],\n",
        "          ['Domain Feature Embedding: case features',acc_2]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 0.8128\n",
            "val acc: 0.8253\n",
            "val acc: 0.8390\n",
            "\n",
            " Models                                                      Accuracy \n",
            "\n",
            " Base model (glove-wiki)                                     0.812802 \n",
            "\n",
            " Syntactic Textual Feature Embedding: PoS tag information    0.82527  \n",
            "\n",
            " Domain Feature Embedding: case features                     0.83904  \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rktE83AZRJTE"
      },
      "source": [
        "**3. Ablation Study - different attention strategy** \n",
        "\n",
        "~~~\n",
        "Model: Bi-lstm+crf\n",
        "\n",
        "Base model: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6 + scaled dot-product + crf + 1 layer\n",
        "\n",
        "Input embedding dimension: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6\n",
        "\n",
        "Epochs: 15 with early stopping\n",
        "\n",
        "Learing rate: 0.01\n",
        "\n",
        "Weight decay: 1e-4\n",
        "\n",
        "Optimiser: SGD\n",
        "\n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 128\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ee-jyEFeRLEK",
        "outputId": "fa22dd85-4929-414b-87dc-394f60a98259"
      },
      "source": [
        "title = ['Models','Accuracy']\n",
        "acc_base = get_accuracy('basemodel-no-attention.pt')\n",
        "acc_dot = get_accuracy('basemodel-dot-product.pt')\n",
        "acc_scaled = get_accuracy('basemodel-scaled-dot-product.pt')\n",
        "acc_general = get_accuracy('basemodel-general.pt')\n",
        "\n",
        "table = [title,\n",
        "         ['Base model (no attention)',acc_base],\n",
        "          ['Dot product:',acc_dot],\n",
        "          ['Scaled dot product:',acc_scaled],\n",
        "          ['General:',acc_general]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 0.8282\n",
            "val acc: 0.8229\n",
            "val acc: 0.8374\n",
            "val acc: 0.8249\n",
            "\n",
            " Models                       Accuracy \n",
            "\n",
            " Base model (no attention)    0.828247 \n",
            "\n",
            " Dot product:                 0.822851 \n",
            "\n",
            " Scaled dot product:          0.837365 \n",
            "\n",
            " General:                     0.824898 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYnudghyRLmu"
      },
      "source": [
        "**4. Ablation Study - different Stacked layer or # of encoder/decoder strategy**\n",
        "\n",
        "~~~\n",
        "Model: Bi-lstm+crf\n",
        "\n",
        "Base model: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6 + scaled dot-product + crf + 1 layer\n",
        "\n",
        "Input embedding dimension: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6\n",
        "\n",
        "Epochs: 15 with early stopping\n",
        "\n",
        "Learing rate: 0.01\n",
        "\n",
        "Weight decay: 1e-4\n",
        "\n",
        "Optimiser: SGD\n",
        "\n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 128\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr5KKpMsRN_w",
        "outputId": "6c6331ba-21c6-4505-9d93-f9c58e54a5f6"
      },
      "source": [
        "title = ['Models','Accuracy']\n",
        "acc_base = get_accuracy('best-model.pt')\n",
        "acc_2 = get_accuracy('basemodel-2-layer')\n",
        "acc_3 = get_accuracy('basemodel-3-layer')\n",
        "\n",
        "table = [title,\n",
        "         ['Base model (1 layer)',acc_base],\n",
        "          ['2 Stacked layer:',acc_2],\n",
        "          ['3 Stacked layer:',acc_3]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 0.8389\n",
            "val acc: 0.8262\n",
            "val acc: 0.8214\n",
            "\n",
            " Models                  Accuracy \n",
            "\n",
            " Base model (1 layer)    0.838854 \n",
            "\n",
            " 2 Stacked layer:        0.8262   \n",
            "\n",
            " 3 Stacked layer:        0.821362 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zk3LCUc1ROnT"
      },
      "source": [
        "**5. Ablation Study - with/without CRF**\n",
        "\n",
        "~~~\n",
        "Model: Bi-lstm+crf\n",
        "\n",
        "Base model: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6 + scaled dot-product + no crf + 1 layer\n",
        "\n",
        "Input embedding dimension: word embedding (glove-wiki) 50 + sentence embedding (pos tag) 4 + domain embedding (case features) 6\n",
        "\n",
        "Epochs: 15 with early stopping\n",
        "\n",
        "Learing rate: 0.01\n",
        "\n",
        "Weight decay: 1e-4\n",
        "\n",
        "Optimiser: SGD\n",
        "\n",
        "LSTM dropout: 0.1\n",
        "\n",
        "Hidden dimensions: 128\n",
        "~~~"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHm4Oj9zRQ0S",
        "outputId": "285d096a-218f-41f3-dfe5-3e3163c1b1aa"
      },
      "source": [
        "title = ['Models','Accuracy']\n",
        "acc_base = get_accuracy('best-model.pt')\n",
        "acc_2 = get_accuracy('basemodel-without-crf')\n",
        "\n",
        "table = [title,\n",
        "         ['Base model (with CRF)',acc_base],\n",
        "          ['Without CRF:',acc_2]]\n",
        "print(tabulate(table, headers='firstrow', tablefmt='fancy_grid'))"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 0.8398\n",
            "val acc: 0.7540\n",
            "\n",
            " Models                   Accuracy \n",
            "\n",
            " Base model (with CRF)    0.839784 \n",
            "\n",
            " Without CRF:             0.754001 \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fLAMD2_0LVtt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}